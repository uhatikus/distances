{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Aleksandr Ukhatov #\n",
    "#####################\n",
    "\n",
    "# imports \n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import ntpath\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from shutil import copyfile\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultError(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def __str__(self):\n",
    "        return repr(self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find matches between to given images\n",
    "# inputs are image names\n",
    "def match_imgs(img1, img2_name, show=False):\n",
    "    \n",
    "#     MIN_MATCH_COUNT = 70\n",
    "\n",
    "    # read images \n",
    "#     img1 = cv2.imread(img1_name,0) # queryImage\n",
    "    img2 = cv2.imread(img2_name,0) # trainImage\n",
    "\n",
    "    # Initiate SIFT detector\n",
    "    # sift = cv2.SIFT()\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "    \n",
    "    if show:\n",
    "        if len(good)>MIN_MATCH_COUNT:\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "\n",
    "            h,w = img1.shape\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            dst = cv2.perspectiveTransform(pts,M)\n",
    "\n",
    "            img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "        else:\n",
    "            print(\"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT))\n",
    "\n",
    "            matchesMask = None\n",
    "\n",
    "        draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                           singlePointColor = None,\n",
    "                           matchesMask = matchesMask, # draw only inliers\n",
    "                           flags = 2)\n",
    "\n",
    "        img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\n",
    "\n",
    "        plt.imshow(img3, 'gray'),plt.show()\n",
    "    \n",
    "    # return number of good matches\n",
    "    return len(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target frames, with target object/car   \n",
    "def get_target_frames(project_dir, img1_name, folder_to=\"target\", folder_from=\"images\"):\n",
    "    # init constants\n",
    "    MIN_MATCH_COUNT = 70\n",
    "    \n",
    "    # make folder for target images\n",
    "    os.system(\"mkdir \" + project_dir + folder_to)\n",
    "    # get name of target/query image \n",
    "    img1_name = ntpath.dirname(project_dir[:-1]) + \"/\" + img1_name\n",
    "    # get names of images which might become target\n",
    "    cropped_imgs =  sorted(glob.glob(project_dir + folder_from + \"/*.jpg\"))\n",
    "    goodness = []\n",
    "    \n",
    "    # use multiprocessing \n",
    "#     pool = mp.Pool(mp.cpu_count())\n",
    "    # find number of matches of all images with target/query image\n",
    "    img1 = cv2.imread(img1_name,0)\n",
    "#     goodness = [pool.apply(match_imgs, args=(img1_name, img)) for img in cropped_imgs]\n",
    "    for img in cropped_imgs:\n",
    "        goodness.append(match_imgs(img1, img))\n",
    "    \n",
    "    max_goodness = max(goodness)\n",
    "    if max_goodness < MIN_MATCH_COUNT:\n",
    "        print(\"Max goodness is \" + str(max_goodness))\n",
    "        print(\"There is no target car in the input video \" + ntpath.basename(project_dir[:-1]) + \".mp4\")\n",
    "        return \n",
    "    \n",
    "    best_index = goodness.index(max_goodness)\n",
    "    new_file_name = project_dir + folder_to + \"/\" + ntpath.basename(cropped_imgs[best_index])\n",
    "    copyfile(cropped_imgs[best_index], new_file_name)\n",
    "    \n",
    "    return new_file_name\n",
    "\n",
    "\n",
    "    # find relevant images, with target on them\n",
    "    good_images = list(map(lambda x: int(x > MIN_MATCH_COUNT), goodness))\n",
    "    \n",
    "#     print(good_images)\n",
    "#     print(goodness)\n",
    "\n",
    "    # for each image decide copy it or not\n",
    "    images_n = len(good_images)\n",
    "    if images_n == 0:\n",
    "        raise ResultError(\"There is no target car in the input videos!\")\n",
    "    for i in range(images_n):\n",
    "        if sum(good_images[max(0, i-(delta)):min(images_n, i+(delta+3))]) > MIN_GOOD_IMAGES_IN_NEIGHBUORS:\n",
    "            # copy of true\n",
    "            copyfile(cropped_imgs[i], project_dir + folder_to + \"/\" + ntpath.basename(cropped_imgs[i]))\n",
    "            \n",
    "    # copy target file to find cluster with turget \n",
    "    copyfile(img1_name, project_dir + folder_to + \"/\" + \"000.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(video_file, pr_dir=0):\n",
    "    # init constants\n",
    "    scale = 0.5\n",
    "    ms = 200\n",
    "    place = \"data/projects2/\"\n",
    "    \n",
    "    print(video_file)\n",
    "\n",
    "    # init project directory\n",
    "    video_name = ((ntpath.basename(video_file)).split(\".\"))[0]\n",
    "    os.system(\"mkdir \" + place)\n",
    "    os.system(\"mkdir \" + place +  video_name)\n",
    "    os.system(\"mkdir \"+ place + video_name + \"/images\")\n",
    "    # save current project directory name\n",
    "    project_dir = os.getcwd() + \"/\" + place + video_name + \"/\" \n",
    "    if pr_dir:\n",
    "        return project_dir\n",
    "    \n",
    "    count = 1\n",
    "    # read image\n",
    "    vidcap = cv2.VideoCapture(video_file)\n",
    "    # read first image\n",
    "    success, image = vidcap.read()\n",
    "    while success:\n",
    "#         print('Read a new frame #' + str(count))\n",
    "        # resize image\n",
    "        dim = (int(image.shape[1] * scale), int(image.shape[0] * scale))\n",
    "        image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "        # save image\n",
    "        cv2.imwrite(project_dir + \"images/\" + video_name + \"_frame_\" + str(count).zfill(3) + \".jpg\", image)     # save frame as JPEG file      \n",
    "        # read new image\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,(count*ms))\n",
    "        success, image = vidcap.read()\n",
    "        # increase count\n",
    "        count += 1  \n",
    "        # limitations\n",
    "        if count > 50:\n",
    "            break \n",
    "    print(\"Obtained \" + str(count-1) + \" images\")\n",
    "    # return current project directory\n",
    "    return project_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_n_images(frames, show=False):\n",
    "        \n",
    "#     MIN_MATCH_COUNT = 70\n",
    "    \n",
    "    n = len(frames)\n",
    "    # read images \n",
    "    imgs = []\n",
    "    for frame in frames:\n",
    "        imgs.append(cv2.imread(frame,0)) # queryImage\n",
    "\n",
    "    # Initiate SIFT detector\n",
    "    # sift = cv2.SIFT()\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kps = []\n",
    "    dess = []\n",
    "    for img in imgs:\n",
    "        kp, des = sift.detectAndCompute(img,None)\n",
    "        kps.append(kp)\n",
    "        dess.append(des)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    matches = {}\n",
    "    num_matches = {}\n",
    "    print(n)\n",
    "    for i in range(n):\n",
    "        matches[i] = {} \n",
    "        num_matches[i] = {}\n",
    "        for j in range(n):\n",
    "            print(j)\n",
    "            j += i + 1\n",
    "            if j >= n:\n",
    "                continue\n",
    "            matches[i][j] = flann.knnMatch(dess[i],dess[j],k=2)\n",
    "\n",
    "            # store all the good matches as per Lowe's ratio test.\n",
    "            good = []\n",
    "            for m,n in matches[i][j]:\n",
    "                if m.distance < 0.7*n.distance:\n",
    "                    good.append(m)\n",
    "\n",
    "            num_matches[i][j] = len(good)\n",
    "            \n",
    "            if show:\n",
    "                if len(good)>MIN_MATCH_COUNT:\n",
    "                    src_pts = np.float32([ kps[i][m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "                    dst_pts = np.float32([ kps[j][m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "                    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "                    matchesMask = mask.ravel().tolist()\n",
    "\n",
    "                    h,w = imgs[i].shape\n",
    "                    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "                    dst = cv2.perspectiveTransform(pts,M)\n",
    "\n",
    "                    img2 = cv2.polylines(imgs[j],[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "                else:\n",
    "                    print(\"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT))\n",
    "\n",
    "                    matchesMask = None\n",
    "\n",
    "                draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                                   singlePointColor = None,\n",
    "                                   matchesMask = matchesMask, # draw only inliers\n",
    "                                   flags = 2)\n",
    "\n",
    "                img3 = cv2.drawMatches(imgs[i],kps[i],img2,kps[j] ,good,None,**draw_params)\n",
    "\n",
    "                plt.imshow(img3, 'gray'),plt.show()\n",
    "\n",
    "            # return number of good matches\n",
    "    return num_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ukhatov/Documents/Projects/distances/data/videos/20190816_134744.mp4\n",
      "Obtained 47 images\n",
      "video is done\n",
      "/Users/ukhatov/Documents/Projects/distances/data/videos/20190816_135049.mp4\n",
      "Obtained 50 images\n",
      "video is done\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "while ((list(filter(bool, ROOT_DIR.split(\"/\"))))[-1] != \"distances\"):\n",
    "    os.chdir('..')\n",
    "    ROOT_DIR = os.getcwd()\n",
    "video_files = []\n",
    "project_dirs = []\n",
    "target_frames = []\n",
    "video_files.append(\"/Users/ukhatov/Documents/Projects/distances/data/videos/20190816_134744.mp4\")\n",
    "video_files.append(\"/Users/ukhatov/Documents/Projects/distances/data/videos/20190816_135049.mp4\")\n",
    "for video_file in video_files:\n",
    "    project_dirs.append(video_to_frames(video_file))\n",
    "    target_frames += [get_target_frames(project_dirs[-1], \"target.jpg\", \"target\", \"images\")]\n",
    "    print(\"video is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'int' and 'cv2.DMatch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9665a9ee048c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_n_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-58c0f5fa25df>\u001b[0m in \u001b[0;36mmatch_n_images\u001b[0;34m(frames, show)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknnMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'int' and 'cv2.DMatch'"
     ]
    }
   ],
   "source": [
    "matches = match_n_images(target_frames, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
